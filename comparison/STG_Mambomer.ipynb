{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### prepare.py\n",
        "\n",
        "import torch.utils.data as utils\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def PrepareDataset(speed_matrix, BATCH_SIZE=48, seq_len=12, pred_len=12, train_propotion=0.7, valid_propotion=0.1):\n",
        "    time_len = speed_matrix.shape[0]\n",
        "    max_speed = speed_matrix.max().max()\n",
        "    min_speed = speed_matrix.min().min()\n",
        "    speed_matrix = (speed_matrix - min_speed)/(max_speed - min_speed)\n",
        "\n",
        "    speed_sequences, speed_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "        speed_sequences.append(speed_matrix.iloc[i:i + seq_len].values)\n",
        "        speed_labels.append(speed_matrix.iloc[i + seq_len:i + seq_len + pred_len].values)\n",
        "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
        "\n",
        "    speed_labels = speed_labels.reshape(speed_labels.shape[0], pred_len, -1)\n",
        "    sample_size = speed_sequences.shape[0]\n",
        "\n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * (train_propotion + valid_propotion)))\n",
        "\n",
        "    train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
        "    valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
        "    test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
        "\n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "\n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader, max_speed"
      ],
      "metadata": {
        "id": "Le-odSqAnKSL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### modules.py\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicFilterGNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features, filter_adjacency_matrix, bias=True):\n",
        "        super(DynamicFilterGNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.base_filter = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        self.filter_adjacency_matrix = None\n",
        "        #self.base_filter = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "        if use_gpu:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix.cuda(), requires_grad=False)\n",
        "        else:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix, requires_grad=False)\n",
        "\n",
        "        self.transform = nn.Linear(in_features, in_features)\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.base_filter.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9*self.filter_adjacency_matrix+0.1*transformed_filter\n",
        "        result_embed = F.linear(input, transformed_adjacency.matmul(self.weight), self.bias)\n",
        "        #F.linear(input, transformed_adjacency.matmul(self.weight), self.bias)\n",
        "        return result_embed\n",
        "\n",
        "\n",
        "    def get_transformed_adjacency(self):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9 * self.filter_adjacency_matrix + 0.1 * transformed_filter\n",
        "        return transformed_adjacency\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'in_features=' + str(self.in_features) \\\n",
        "               + ', out_features=' + str(self.out_features) \\\n",
        "               + ', bias=' + str(self.bias is not None) + ')'\n",
        "\n"
      ],
      "metadata": {
        "id": "uQFJtmPznlfq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STGMamba.py\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "# KFGN (Kalman Filtering Graph Neural Networks) Model\n",
        "class KFGN(nn.Module):\n",
        "    def __init__(self, K, A, feature_size, Clamp_A=True):\n",
        "        super(KFGN, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.hidden_size = feature_size\n",
        "        self.K = K\n",
        "        self.A_list = []\n",
        "\n",
        "        D_inverse = torch.diag(1 / torch.sum(A, 0))\n",
        "        norm_A = torch.matmul(D_inverse, A)\n",
        "        A = norm_A\n",
        "\n",
        "        A_temp = torch.eye(feature_size, feature_size)\n",
        "        for i in range(K):\n",
        "            A_temp = torch.matmul(A_temp, A)\n",
        "            if Clamp_A:\n",
        "                A_temp = torch.clamp(A_temp, max=1.)\n",
        "            self.A_list.append(A_temp)\n",
        "\n",
        "        self.gc_list = nn.ModuleList([DynamicFilterGNN(feature_size, feature_size, self.A_list[i], bias=False) for i in range(K)])\n",
        "        hidden_size = self.feature_size\n",
        "        gc_input_size = self.feature_size * K\n",
        "\n",
        "        self.fl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "        self.Neighbor_weight = Parameter(torch.FloatTensor(feature_size))\n",
        "        stdv = 1. / math.sqrt(feature_size)\n",
        "        self.Neighbor_weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "        input_size = self.feature_size\n",
        "\n",
        "        self.rfl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ril = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rol = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rCl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # addtional vars\n",
        "        self.c = torch.nn.Parameter(torch.Tensor([1]))\n",
        "\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, 64)\n",
        "        self.fc5 = nn.Linear(64, hidden_size)\n",
        "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc8 = nn.Linear(hidden_size, 64)\n",
        "\n",
        "    def forward(self, input, Hidden_State=None, Cell_State=None, rHidden_State=None, rCell_State=None):\n",
        "        batch_size, time_steps, _ = input.shape\n",
        "        if Hidden_State is None:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if Cell_State is None:\n",
        "            Cell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rHidden_State is None:\n",
        "            rHidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rCell_State is None:\n",
        "            rCell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "\n",
        "        Hidden_State = Hidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        Cell_State = Cell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rHidden_State = rHidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rCell_State = rCell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "\n",
        "        x = input\n",
        "        gc = self.gc_list[0](x)\n",
        "        for i in range(1, self.K):\n",
        "            gc = torch.cat((gc, self.gc_list[i](x)), 1)\n",
        "\n",
        "        combined = torch.cat((gc, Hidden_State), 1)\n",
        "        dim1=combined.shape[0]\n",
        "        dim2=combined.shape[1]\n",
        "        dim3=combined.shape[2]\n",
        "        combined=combined.view(dim1,dim2//4,dim3*4)\n",
        "\n",
        "        f = torch.sigmoid(self.fl(combined))\n",
        "        i = torch.sigmoid(self.il(combined))\n",
        "        o = torch.sigmoid(self.ol(combined))\n",
        "        C = torch.tanh(self.Cl(combined))\n",
        "\n",
        "        NC = torch.mul(Cell_State,\n",
        "                       torch.mv(Variable(self.A_list[-1], requires_grad=False).cuda(), self.Neighbor_weight))\n",
        "        Cell_State = f * NC + i * C\n",
        "        Hidden_State = o * torch.tanh(Cell_State)\n",
        "\n",
        "        # LSTM\n",
        "        rcombined = torch.cat((input, rHidden_State), 1)\n",
        "        d1=rcombined.shape[0]\n",
        "        d2=rcombined.shape[1]\n",
        "        d3=rcombined.shape[2]\n",
        "        rcombined=rcombined.view(d1,d2//2,d3*2)\n",
        "        rf = torch.sigmoid(self.rfl(rcombined))\n",
        "        ri = torch.sigmoid(self.ril(rcombined))\n",
        "        ro = torch.sigmoid(self.rol(rcombined))\n",
        "        rC = torch.tanh(self.rCl(rcombined))\n",
        "        rCell_State = rf * rCell_State + ri * rC\n",
        "        rHidden_State = ro * torch.tanh(rCell_State)\n",
        "\n",
        "        # Kalman Filtering\n",
        "        var1, var2 = torch.var(input), torch.var(gc)\n",
        "\n",
        "        pred = (Hidden_State * var1 * self.c + rHidden_State * var2) / (var1 + var2 * self.c)\n",
        "\n",
        "        return pred\n",
        "        #return Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred\n",
        "\n",
        "    def Bi_torch(self, a):\n",
        "        a[a < 0] = 0\n",
        "        a[a > 0] = 1\n",
        "        return a\n",
        "\n",
        "    def loop(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        time_step = inputs.size(1)\n",
        "        Hidden_State, Cell_State, rHidden_State, rCell_State = self.initHidden(batch_size)\n",
        "        for i in range(time_step):\n",
        "            Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred = self.forward(\n",
        "                torch.squeeze(inputs[:, i:i + 1, :]), Hidden_State, Cell_State, rHidden_State, rCell_State)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "    def reinitHidden(self, batch_size, Hidden_State_data, Cell_State_data):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(Hidden_State_data, requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data, requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "# Mamba Network\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    features: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_conv: int = 4\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "    K: int = 3\n",
        "    A: torch.Tensor = None\n",
        "    feature_size: int = None\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "class KFGN_Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.encode = nn.Linear(args.features, args.d_model)\n",
        "        self.encoder_layers = nn.ModuleList([ResidualBlock(args,self.kfgn) for _ in range(args.n_layer)])\n",
        "        self.encoder_norm = RMSNorm(args.d_model)\n",
        "        # Decoder (identical to Encoder)\n",
        "        ##self.decoder_layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)]) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##self.decoder_norm = RMSNorm(args.d_model) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        self.decode = nn.Linear(args.d_model, args.features)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.encode(input_ids)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.encoder_norm(x)\n",
        "        # Decoder\n",
        "        ##for layer in self.decoder_layers:#You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##    x = layer(x) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##x = self.decoder_norm(x) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "\n",
        "        # Output\n",
        "        x = self.decode(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Residual Block in Mamba Model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.mixer = MambaBlock(args,kfgn)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x1 = self.norm(x)\n",
        "        x2 = self.kfgn(x1)\n",
        "        x3 = self.mixer(x2)\n",
        "        output = x3 + x1\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = kfgn\n",
        "        self.gge = GraphormerGraphEncoder()\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 3, bias=False)\n",
        "\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)\n",
        "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "\n",
        "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x):\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)\n",
        "\n",
        "        (delta, B, B_att, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n, n], dim=-1)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
        "\n",
        "        att = self.gge(x)[0]\n",
        "\n",
        "        # y = self.selective_scan(x, delta, A, B, C, D)\n",
        "        y = self.selective_scan(x, delta, A, B, B_att, C, D, att)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, B_att, C, D, att):\n",
        "    # def selective_scan(self, u, delta, A, B, C, D):\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "        # This is the new version of Selective Scan Algorithm named as \"Graph Selective Scan\"\n",
        "        #In Graph Selective Scan, we use the Feed-Forward graph information from KFGN, and incorporate the Feed-Forward information with \"delta\"\n",
        "        temp_adj = self.kfgn.gc_list[-1].get_transformed_adjacency()\n",
        "        temp_adj_padded = torch.ones(d_in, d_in, device=temp_adj.device)\n",
        "        temp_adj_padded[:temp_adj.size(0), :temp_adj.size(1)] = temp_adj\n",
        "\n",
        "        delta_p = torch.matmul(delta,temp_adj_padded)\n",
        "\n",
        "        att = att.squeeze()\n",
        "        att = torch.Tensor(att).to('cuda')\n",
        "        att = att.permute(1, 0, 2)\n",
        "\n",
        "\n",
        "        # The fused param delta_p will participate in the following upgrading of deltaA and deltaB_u\n",
        "        deltaA = torch.exp(einsum(delta_p, A, 'b l d_in, d_in n -> b l d_in n'))\n",
        "        deltaB_u = einsum(delta_p, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "        deltaC_a = einsum(delta_p, B_att, att, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i] + deltaC_a[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "\n",
        "        y = y + u * D\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2LGB4WYWnek0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, Union, List, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import LayerNorm\n",
        "\n",
        "# from src.modules.graphormer_layers import GraphNodeFeature, GraphAttnBias\n",
        "\n",
        "\n",
        "class GraphormerGraphEncoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            # < graph\n",
        "            static_graph: bool = True,\n",
        "            graph_token=False,\n",
        "            # >\n",
        "            # transformer\n",
        "            num_encoder_layers: int = 12,\n",
        "            embedding_dim: int = 600,\n",
        "            ffn_embedding_dim: int = 600,\n",
        "            num_attention_heads: int = 30,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            encoder_normalize_before: bool = True,\n",
        "            pre_layernorm: bool = True,\n",
        "            apply_graphormer_init: bool = False,\n",
        "            activation_fn: str = \"gelu\",\n",
        "            embed_scale: float = None,\n",
        "            freeze_embeddings: bool = False,\n",
        "            n_trans_layers_to_freeze: int = 0,\n",
        "            export: bool = False,\n",
        "            traceable: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.static_graph = static_graph\n",
        "        self.graph_token = graph_token\n",
        "\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embed_scale = embed_scale\n",
        "        self.traceable = traceable\n",
        "        self.apply_graphormer_init = apply_graphormer_init\n",
        "\n",
        "        if encoder_normalize_before:\n",
        "            self.emb_layer_norm = LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "        else:\n",
        "            self.emb_layer_norm = None\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.layers.extend(\n",
        "            [\n",
        "                GraphormerGraphEncoderLayer(\n",
        "                    embedding_dim=embedding_dim,\n",
        "                    ffn_embedding_dim=ffn_embedding_dim,\n",
        "                    num_attention_heads=num_attention_heads,\n",
        "                    dropout=dropout,\n",
        "                    attention_dropout=attention_dropout,\n",
        "                    activation_dropout=activation_dropout,\n",
        "                    activation_fn=activation_fn,\n",
        "                    export=export,\n",
        "                    pre_layernorm=pre_layernorm,\n",
        "                )\n",
        "                for _ in range(num_encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        def freeze_module_params(m):\n",
        "            if m is not None:\n",
        "                for p in m.parameters():\n",
        "                    p.requires_grad = False\n",
        "\n",
        "\n",
        "        for layer in range(n_trans_layers_to_freeze):\n",
        "            freeze_module_params(self.layers[layer])\n",
        "\n",
        "    def compute_attn_bias(self, batched_data):\n",
        "        attn_bias = self.graph_attn_bias(batched_data)\n",
        "        return attn_bias\n",
        "\n",
        "\n",
        "    def forward_transformer_layers(\n",
        "            self,\n",
        "            x,\n",
        "            padding_mask,\n",
        "            attn_bias=None,\n",
        "            attn_mask=None,\n",
        "            last_state_only=True,\n",
        "            get_attn_scores=False,\n",
        "    ):\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.contiguous().transpose(0, 1)\n",
        "\n",
        "        inner_states, attn_scores = [], []\n",
        "        if not last_state_only:\n",
        "            inner_states.append(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(\n",
        "                x,\n",
        "                self_attn_padding_mask=padding_mask,\n",
        "                self_attn_mask=attn_mask,\n",
        "                self_attn_bias=attn_bias,\n",
        "                get_attn_scores=get_attn_scores,\n",
        "            )\n",
        "            if not last_state_only:\n",
        "                inner_states.append(x)\n",
        "                attn_scores.append(attn)\n",
        "\n",
        "        if last_state_only:\n",
        "            inner_states = [x]\n",
        "            attn_scores = [attn]\n",
        "        return inner_states, attn_scores\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x,\n",
        "            attn_bias=None,\n",
        "            # perturb=None,\n",
        "            last_state_only: bool = True,\n",
        "            # token_embeddings: Optional[torch.Tensor] = None,\n",
        "            attn_mask: Optional[torch.Tensor] = None,\n",
        "            get_attn_scores=False,\n",
        "    ) -> Union[Tensor, list[torch.Tensor]]:\n",
        "        if get_attn_scores:\n",
        "            last_state_only = False\n",
        "        # compute padding mask. This is needed for multi-head attention\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        padding_mask = None\n",
        "        # padding_mask = (x[:, :, 0]).eq(0)  # B x T x 1\n",
        "        # bug in the original code corrected below:\n",
        "        # padding_mask = torch.all(x[:,:,].eq(0), dim=-1)\n",
        "        # if self.graph_token:\n",
        "        #     padding_mask_graph_tok = torch.zeros(\n",
        "        #         N, 1, device=padding_mask.device, dtype=padding_mask.dtype\n",
        "        #     )\n",
        "        #     padding_mask = torch.cat((padding_mask_graph_tok, padding_mask), dim=1)\n",
        "        # else:\n",
        "        #     padding_mask_cls = torch.zeros(\n",
        "        #         N, 1, device=padding_mask.device, dtype=padding_mask.dtype\n",
        "        #     )\n",
        "        #     padding_mask = torch.cat((padding_mask_cls, padding_mask), dim=1)\n",
        "        # B x (T+1) x 1\n",
        "\n",
        "        if self.embed_scale is not None:\n",
        "            x = x * self.embed_scale\n",
        "        if self.emb_layer_norm is not None:\n",
        "            x = self.emb_layer_norm(x)\n",
        "        x = self.dropout_module(x)\n",
        "\n",
        "        inner_states, attn_scores = self.forward_transformer_layers(\n",
        "            x=x,\n",
        "            padding_mask=padding_mask,\n",
        "            attn_bias=attn_bias,\n",
        "            attn_mask=attn_mask,\n",
        "            last_state_only=last_state_only,\n",
        "            get_attn_scores=get_attn_scores,\n",
        "        )\n",
        "\n",
        "        inner_states = np.array([t.detach().cpu().numpy() for t in inner_states])\n",
        "        attn_scores = np.array(attn_scores)\n",
        "\n",
        "\n",
        "        if not last_state_only:\n",
        "            return torch.stack(inner_states), torch.stack(attn_scores)\n",
        "        else:\n",
        "            return inner_states, attn_scores"
      ],
      "metadata": {
        "id": "wcQPreHx0pcd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import LayerNorm\n",
        "\n",
        "\n",
        "class GraphormerGraphEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: int = 300,\n",
        "            ffn_embedding_dim: int = 300,\n",
        "            num_attention_heads: int = 1,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = \"gelu\",\n",
        "            export: bool = False,\n",
        "            init_fn: Callable = None,\n",
        "            pre_layernorm: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if init_fn is not None:\n",
        "            init_fn()\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.pre_layernorm = pre_layernorm\n",
        "\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "        self.activation_dropout_module = nn.Dropout(activation_dropout)\n",
        "\n",
        "        self.activation_fn = nn.GELU() if activation_fn == \"gelu\" else nn.ReLU()\n",
        "\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=True,\n",
        "        )\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim, eps=1e-8)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            self_attn_bias: Optional[torch.Tensor] = None,\n",
        "            self_attn_mask: Optional[torch.Tensor] = None,\n",
        "            self_attn_padding_mask: Optional[torch.Tensor] = None,\n",
        "            get_attn_scores=False,\n",
        "    ):\n",
        "\n",
        "        # x: T x B x C\n",
        "        residual = x\n",
        "        if self.pre_layernorm:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        x, attn = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            attn_bias=self_attn_bias,\n",
        "            key_padding_mask=self_attn_padding_mask,\n",
        "            need_weights=get_attn_scores,\n",
        "            attn_mask=self_attn_mask,\n",
        "        )\n",
        "        x = self.dropout_module(x)\n",
        "        x = residual + x\n",
        "        if not self.pre_layernorm:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.pre_layernorm:\n",
        "            x = self.final_layer_norm(x)\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = self.activation_dropout_module(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout_module(x)\n",
        "        x = residual + x\n",
        "        if not self.pre_layernorm:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return x, attn"
      ],
      "metadata": {
        "id": "H3XPN_60oyOP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            self_attention=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "\n",
        "        assert self.self_attention, \"Only support self attention\"\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        self.k_proj = nn.Linear(self.kdim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(self.vdim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            attn_bias: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            need_weights: bool = True,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "\n",
        "        if need_head_weights:\n",
        "            need_weights = True\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        src_len = tgt_len\n",
        "        assert embed_dim == self.embed_dim, f\"query dim {embed_dim} != {self.embed_dim}\"\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        q = self.q_proj(query)\n",
        "        k = self.k_proj(query)\n",
        "        v = self.v_proj(query)\n",
        "        q *= self.scaling\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n",
        "            .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "        attn_weights = torch.bmm(q, k.contiguous().transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_bias is not None:\n",
        "            attn_weights += attn_bias.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.masked_fill(\n",
        "                key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                float(\"-inf\"),\n",
        "            )\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.contiguous().view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights\n",
        "\n",
        "    def upgrade_state_dict_named(self, state_dict, name):\n",
        "        prefix = name + \".\" if name != \"\" else \"\"\n",
        "        items_to_add = {}\n",
        "        keys_to_remove = []\n",
        "        for k in state_dict.keys():\n",
        "            if k.endswith(prefix + \"in_proj_weight\"):\n",
        "                # in_proj_weight used to be q + k + v with same dimensions\n",
        "                dim = int(state_dict[k].shape[0] / 3)\n",
        "                items_to_add[prefix + \"q_proj.weight\"] = state_dict[k][:dim]\n",
        "                items_to_add[prefix + \"k_proj.weight\"] = state_dict[k][dim: 2 * dim]\n",
        "                items_to_add[prefix + \"v_proj.weight\"] = state_dict[k][2 * dim:]\n",
        "\n",
        "                keys_to_remove.append(k)\n",
        "\n",
        "                k_bias = prefix + \"in_proj_bias\"\n",
        "                if k_bias in state_dict.keys():\n",
        "                    dim = int(state_dict[k].shape[0] / 3)\n",
        "                    items_to_add[prefix + \"q_proj.bias\"] = state_dict[k_bias][:dim]\n",
        "                    items_to_add[prefix + \"k_proj.bias\"] = state_dict[k_bias][\n",
        "                                                           dim: 2 * dim\n",
        "                                                           ]\n",
        "                    items_to_add[prefix + \"v_proj.bias\"] = state_dict[k_bias][2 * dim:]\n",
        "\n",
        "                    keys_to_remove.append(prefix + \"in_proj_bias\")\n",
        "\n",
        "        for k in keys_to_remove:\n",
        "            del state_dict[k]\n",
        "\n",
        "        for key, value in items_to_add.items():\n",
        "            state_dict[key] = value"
      ],
      "metadata": {
        "id": "JAM3GhAykpzj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### train_STGmamba.py\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.autograd import Variable\n",
        "\n",
        "y = []  # by me\n",
        "\n",
        "def TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=1, mamba_features=300):\n",
        "    # 'mamba_features=184' if we use Knowair dataset;\n",
        "    # 'mamba_features=307' if we use PEMS04 datastet;\n",
        "    # 'mamba_features=80' if we use HZ_Metro dataset;\n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = fea_size\n",
        "\n",
        "    kfgn_mamba_args = ModelArgs(\n",
        "        K=K,\n",
        "        A=torch.Tensor(A),\n",
        "        feature_size=A.shape[0],\n",
        "        d_model=fea_size,  # hidden_dim is fea_size\n",
        "        n_layer=4,\n",
        "        features=mamba_features\n",
        "    )\n",
        "\n",
        "    kfgn_mamba = KFGN_Mamba(kfgn_mamba_args)\n",
        "    kfgn_mamba.cuda()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = optim.AdamW(kfgn_mamba.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_interval_train = []\n",
        "    losses_valid = []\n",
        "    losses_interval_valid = []\n",
        "    losses_epoch = []  # Initialize the list for epoch losses\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        trained_number = 0\n",
        "\n",
        "        valid_dataloader_iter = iter(valid_dataloader)\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if inputs.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            kfgn_mamba.zero_grad()\n",
        "\n",
        "            labels = torch.squeeze(labels)\n",
        "            pred = kfgn_mamba(inputs)  # Updated to use new model directly\n",
        "\n",
        "            loss_train = loss_MSE(pred, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            # Update learning rate by CosineAnnealingLR\n",
        "            scheduler.step()\n",
        "\n",
        "            losses_train.append(loss_train.data)\n",
        "\n",
        "            # validation\n",
        "            try:\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "            except StopIteration:\n",
        "                valid_dataloader_iter = iter(valid_dataloader)\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "            else:\n",
        "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "            labels_val = torch.squeeze(labels_val)\n",
        "\n",
        "            pred = kfgn_mamba(inputs_val)\n",
        "            loss_valid = loss_MSE(pred, labels_val)\n",
        "            losses_valid.append(loss_valid.data)\n",
        "\n",
        "            trained_number += 1\n",
        "\n",
        "            if trained_number % interval == 0:\n",
        "                cur_time = time.time()\n",
        "                loss_interval_train = np.around(sum(losses_train[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_train.append(loss_interval_train)\n",
        "                loss_interval_valid = np.around(sum(losses_valid[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_valid.append(loss_interval_valid)\n",
        "                print('Iteration #: {}, train_loss: {}, valid_loss: {}, time: {}'.format(\n",
        "                    trained_number * batch_size,\n",
        "                    loss_interval_train,\n",
        "                    loss_interval_valid,\n",
        "                    np.around([cur_time - pre_time], decimals=8)))\n",
        "                pre_time = cur_time\n",
        "\n",
        "        loss_epoch = loss_valid.cpu().data.numpy()\n",
        "        losses_epoch.append(loss_epoch)\n",
        "\n",
        "    return kfgn_mamba, [losses_train, losses_interval_train, losses_valid, losses_interval_valid]\n",
        "\n",
        "\n",
        "\n",
        "def TestSTG_Mamba(kfgn_mamba, test_dataloader, max_speed):\n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    tested_batch = 0\n",
        "\n",
        "    losses_mse = []\n",
        "    losses_l1 = []\n",
        "    MAEs = []\n",
        "    MAPEs = []\n",
        "    MSEs = []\n",
        "    RMSEs = []\n",
        "    VARs = []\n",
        "\n",
        "    #predictions = []\n",
        "    #ground_truths = []\n",
        "\n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        pred = kfgn_mamba(inputs)\n",
        "        labels = torch.squeeze(labels)\n",
        "\n",
        "        loss_mse = F.mse_loss(pred, labels)\n",
        "        loss_l1 = F.l1_loss(pred, labels)\n",
        "        MAE = torch.mean(torch.abs(pred - torch.squeeze(labels)))\n",
        "        MAPE = torch.mean(torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels)))\n",
        "        # Calculate MAPE only for non-zero labels\n",
        "        non_zero_labels = torch.abs(labels) > 0\n",
        "        if torch.any(non_zero_labels):\n",
        "            MAPE_values = torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels))\n",
        "            MAPE = torch.mean(MAPE_values[non_zero_labels])\n",
        "            MAPEs.append(MAPE.item())\n",
        "\n",
        "        MSE = torch.mean((torch.squeeze(labels) - pred)**2)\n",
        "        RMSE = math.sqrt(torch.mean((torch.squeeze(labels) - pred)**2))\n",
        "        VAR = 1-(torch.var(torch.squeeze(labels)-pred))/torch.var(torch.squeeze(labels))\n",
        "\n",
        "        losses_mse.append(loss_mse.item())\n",
        "        losses_l1.append(loss_l1.item())\n",
        "        MAEs.append(MAE.item())\n",
        "        MAPEs.append(MAPE.item())\n",
        "        MSEs.append(MSE.item())\n",
        "        RMSEs.append(RMSE)\n",
        "        VARs.append(VAR.item())\n",
        "\n",
        "        # Reshape pred to 2D before creating DataFrame\n",
        "        #predictions.append(pd.DataFrame(pred.cpu().data.numpy().reshape(-1, fea_size)))\n",
        "        #ground_truths.append(pd.DataFrame(labels.cpu().data.numpy()))\n",
        "        y.append(pred.cpu().data.numpy())  # by me\n",
        "\n",
        "        tested_batch += 1\n",
        "\n",
        "        if tested_batch % 100 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format(\n",
        "                tested_batch * batch_size,\n",
        "                np.around([loss_l1.data[0]], decimals=8),\n",
        "                np.around([loss_mse.data[0]], decimals=8),\n",
        "                np.around([cur_time - pre_time], decimals=8)))\n",
        "            pre_time = cur_time\n",
        "\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    MAEs = np.array(MAEs)\n",
        "    MAPEs = np.array(MAPEs)\n",
        "    MSEs = np.array(MSEs)\n",
        "    RMSEs = np.array(RMSEs)\n",
        "    VARs = np.array(VARs)\n",
        "\n",
        "    mean_l1 = np.mean(losses_l1) * max_speed\n",
        "    std_l1 = np.std(losses_l1) * max_speed\n",
        "    mean_mse = np.mean(losses_mse) * max_speed\n",
        "    MAE_ = np.mean(MAEs) * max_speed\n",
        "    std_MAE_ = np.std(MAEs) * max_speed\n",
        "    MAPE_ = np.mean(MAPEs) * 100\n",
        "    MSE_ = np.mean(MSEs) * (max_speed ** 2)\n",
        "    RMSE_ = np.mean(RMSEs) * max_speed\n",
        "    VAR_ = np.mean(VARs)\n",
        "    results = [MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_]\n",
        "\n",
        "    print('Tested: MAE: {}, std_MAE: {}, MAPE: {}, MSE: {}, RMSE: {}, VAR: {}'.format(MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_))\n",
        "    return results"
      ],
      "metadata": {
        "id": "78QzalCqjAlB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\nLoading PEMS04 data...\")\n",
        "speed_matrix = pd.read_csv('pems04_flow.csv',sep=',')\n",
        "A = np.load('pems04_adj.npy')\n",
        "speed_matrix = speed_matrix.iloc[:, :-7]\n",
        "A = A[:-7, :-7]\n",
        "\n",
        "print(\"\\nPreparing train/test data...\")\n",
        "train_dataloader, valid_dataloader, test_dataloader, max_value = PrepareDataset(speed_matrix, BATCH_SIZE=48)\n",
        "\n",
        "print(\"\\nTraining STGmambomer model...\")\n",
        "STGmamba, STGmamba_loss = TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=50, mamba_features=300)\n",
        "print(\"\\nTesting STGmambomer model...\")\n",
        "results = TestSTG_Mamba(STGmamba, test_dataloader, max_value)"
      ],
      "metadata": {
        "id": "z4YRyEW8e_-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726d77a1-11d7-454d-9a58-632db9e3a057"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading PEMS04 data...\n",
            "\n",
            "Preparing train/test data...\n",
            "\n",
            "Training STGmambomer model...\n",
            "Epoch 0/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.04911648854613304, valid_loss: 0.04581860080361366, time: [32.2637589]\n",
            "Epoch 1/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.009740250185132027, valid_loss: 0.010247609578073025, time: [32.74512577]\n",
            "Epoch 2/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.006450599990785122, valid_loss: 0.006913499906659126, time: [31.87362933]\n",
            "Epoch 3/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.004371330142021179, valid_loss: 0.004787030164152384, time: [32.31834865]\n",
            "Epoch 4/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0036752899177372456, valid_loss: 0.004066199995577335, time: [31.92764688]\n",
            "Epoch 5/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.003323510056361556, valid_loss: 0.003721070010215044, time: [32.12283826]\n",
            "Epoch 6/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.003104679984971881, valid_loss: 0.00351510988548398, time: [31.75261927]\n",
            "Epoch 7/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0029718501027673483, valid_loss: 0.003360169939696789, time: [32.30677843]\n",
            "Epoch 8/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0028262899722903967, valid_loss: 0.0031999798957258463, time: [31.92178869]\n",
            "Epoch 9/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002656159922480583, valid_loss: 0.0030430799815803766, time: [32.27093482]\n",
            "Epoch 10/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0025738601107150316, valid_loss: 0.002944180043414235, time: [31.98432517]\n",
            "Epoch 11/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0024834698997437954, valid_loss: 0.002886550035327673, time: [32.52752566]\n",
            "Epoch 12/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002441009972244501, valid_loss: 0.002836870029568672, time: [31.94079041]\n",
            "Epoch 13/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0023664000909775496, valid_loss: 0.002770849969238043, time: [32.40541697]\n",
            "Epoch 14/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002295420039445162, valid_loss: 0.0027159699238836765, time: [32.02987289]\n",
            "Epoch 15/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002245140029117465, valid_loss: 0.0026809698902070522, time: [32.60054708]\n",
            "Epoch 16/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0022238700184971094, valid_loss: 0.002659670077264309, time: [32.3484726]\n",
            "Epoch 17/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021997399162501097, valid_loss: 0.002653910079970956, time: [32.05690908]\n",
            "Epoch 18/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021295000333338976, valid_loss: 0.002570959972217679, time: [32.55121851]\n",
            "Epoch 19/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002102480037137866, valid_loss: 0.0025643599219620228, time: [32.03400707]\n",
            "Epoch 20/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020605199970304966, valid_loss: 0.0025150799192488194, time: [32.60284662]\n",
            "Epoch 21/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020593099761754274, valid_loss: 0.0025157700292766094, time: [32.06805277]\n",
            "Epoch 22/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002007730072364211, valid_loss: 0.0024752600584179163, time: [32.74710107]\n",
            "Epoch 23/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019788299687206745, valid_loss: 0.0024505299516022205, time: [32.36087298]\n",
            "Epoch 24/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019413100089877844, valid_loss: 0.002418129937723279, time: [32.5629828]\n",
            "Epoch 25/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019441399490460753, valid_loss: 0.0024070199579000473, time: [32.41096115]\n",
            "Epoch 26/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019040299812331796, valid_loss: 0.002387440064921975, time: [32.36736274]\n",
            "Epoch 27/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018942500464618206, valid_loss: 0.002386529929935932, time: [32.59143472]\n",
            "Epoch 28/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018670499557629228, valid_loss: 0.002371859969571233, time: [32.09280396]\n",
            "Epoch 29/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018603099742904305, valid_loss: 0.0023671300150454044, time: [32.66915989]\n",
            "Epoch 30/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00183681002818048, valid_loss: 0.002343389904126525, time: [32.09837484]\n",
            "Epoch 31/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018083100439980626, valid_loss: 0.0023116699885576963, time: [32.48827529]\n",
            "Epoch 32/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018262299709022045, valid_loss: 0.0023373400326818228, time: [31.98223877]\n",
            "Epoch 33/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001786740031093359, valid_loss: 0.002301630098372698, time: [32.24399209]\n",
            "Epoch 34/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001762300031259656, valid_loss: 0.0022922600619494915, time: [32.23610044]\n",
            "Epoch 35/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017605300527065992, valid_loss: 0.002291919896379113, time: [32.29037809]\n",
            "Epoch 36/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017380999634042382, valid_loss: 0.0022572900634258986, time: [32.51982284]\n",
            "Epoch 37/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017327700043097138, valid_loss: 0.002264489885419607, time: [32.16750336]\n",
            "Epoch 38/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017050700262188911, valid_loss: 0.002243960043415427, time: [32.64489293]\n",
            "Epoch 39/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016964300302788615, valid_loss: 0.0022362000308930874, time: [32.17080808]\n",
            "Epoch 40/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016829000087454915, valid_loss: 0.002227070042863488, time: [32.52369642]\n",
            "Epoch 41/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00167582998983562, valid_loss: 0.002214039908722043, time: [32.21191502]\n",
            "Epoch 42/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016483799554407597, valid_loss: 0.0022024300415068865, time: [32.65011358]\n",
            "Epoch 43/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016473799478262663, valid_loss: 0.0022034700959920883, time: [32.53188968]\n",
            "Epoch 44/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016246699960902333, valid_loss: 0.0021897300612181425, time: [32.31534076]\n",
            "Epoch 45/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016072900034487247, valid_loss: 0.0021759800147265196, time: [32.77287316]\n",
            "Epoch 46/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001632779953069985, valid_loss: 0.0021982400212436914, time: [32.24334788]\n",
            "Epoch 47/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0015895500546321273, valid_loss: 0.0021649599075317383, time: [32.58563209]\n",
            "Epoch 48/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0015723099932074547, valid_loss: 0.0021426500752568245, time: [32.34219122]\n",
            "Epoch 49/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0015824600122869015, valid_loss: 0.0021549700759351254, time: [32.7326107]\n",
            "\n",
            "Testing STGmambomer model...\n",
            "Tested: MAE: 22.169304197525673, std_MAE: 1.0147708251605116, MAPE: 18.159656380784924, MSE: 1146.943171298862, RMSE: 33.83884782681866, VAR: 0.9454714121489689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3y9Zamr16B_E"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}