{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### prepare.py\n",
        "\n",
        "import torch.utils.data as utils\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def PrepareDataset(speed_matrix, BATCH_SIZE=48, seq_len=12, pred_len=12, train_propotion=0.7, valid_propotion=0.1):\n",
        "    \"\"\" Prepare Train & Test datasets and dataloaders\n",
        "\n",
        "    Convert traffic/weather/volume matrix to train and test dataset.\n",
        "\n",
        "    Args:\n",
        "        speed_matrix: The whole spatial-temporal dataset matrix. (It doesn't necessarily means speed, but can also be flow or weather matrix).\n",
        "        seq_len: The length of input sequence.\n",
        "        pred_len: The length of prediction sequence, match the seq_len for model compatibility.\n",
        "    Return:\n",
        "        Train_dataloader\n",
        "        Test_dataloader\n",
        "    \"\"\"\n",
        "    time_len = speed_matrix.shape[0]\n",
        "    #max_speed = speed_matrix.max().max()\n",
        "    #speed_matrix = speed_matrix / max_speed\n",
        "\n",
        "    # MinMax Normalization Method.\n",
        "    max_speed = speed_matrix.max().max()\n",
        "    min_speed = speed_matrix.min().min()\n",
        "    speed_matrix = (speed_matrix - min_speed)/(max_speed - min_speed)\n",
        "\n",
        "    speed_sequences, speed_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "        speed_sequences.append(speed_matrix.iloc[i:i + seq_len].values)\n",
        "        speed_labels.append(speed_matrix.iloc[i + seq_len:i + seq_len + pred_len].values)\n",
        "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
        "\n",
        "    # Reshape labels to have the same second dimension as the sequences\n",
        "    speed_labels = speed_labels.reshape(speed_labels.shape[0], seq_len, -1)\n",
        "\n",
        "    # shuffle & split the dataset to training and testing sets\n",
        "    sample_size = speed_sequences.shape[0]\n",
        "    index = np.arange(sample_size, dtype=int)\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * (train_propotion + valid_propotion)))\n",
        "\n",
        "    train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
        "    valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
        "    test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
        "\n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "\n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader, max_speed"
      ],
      "metadata": {
        "id": "Le-odSqAnKSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### modules.py\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicFilterGNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features, filter_adjacency_matrix, bias=True):\n",
        "        super(DynamicFilterGNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.base_filter = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        self.filter_adjacency_matrix = None\n",
        "        #self.base_filter = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "        if use_gpu:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix.cuda(), requires_grad=False)\n",
        "        else:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix, requires_grad=False)\n",
        "\n",
        "        self.transform = nn.Linear(in_features, in_features)\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.base_filter.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9*self.filter_adjacency_matrix+0.1*transformed_filter\n",
        "        result_embed = F.linear(input, transformed_adjacency.matmul(self.weight), self.bias)\n",
        "        #F.linear(input, transformed_adjacency.matmul(self.weight), self.bias)\n",
        "        return result_embed\n",
        "\n",
        "\n",
        "    def get_transformed_adjacency(self):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9 * self.filter_adjacency_matrix + 0.1 * transformed_filter\n",
        "        return transformed_adjacency\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'in_features=' + str(self.in_features) \\\n",
        "               + ', out_features=' + str(self.out_features) \\\n",
        "               + ', bias=' + str(self.bias is not None) + ')'\n",
        "\n"
      ],
      "metadata": {
        "id": "uQFJtmPznlfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STGMamba.py\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "# KFGN (Kalman Filtering Graph Neural Networks) Model\n",
        "class KFGN(nn.Module):\n",
        "    def __init__(self, K, A, feature_size, Clamp_A=True):\n",
        "        super(KFGN, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.hidden_size = feature_size\n",
        "        self.K = K\n",
        "        self.A_list = []\n",
        "\n",
        "        D_inverse = torch.diag(1 / torch.sum(A, 0))\n",
        "        norm_A = torch.matmul(D_inverse, A)\n",
        "        A = norm_A\n",
        "\n",
        "        A_temp = torch.eye(feature_size, feature_size)\n",
        "        for i in range(K):\n",
        "            A_temp = torch.matmul(A_temp, A)\n",
        "            if Clamp_A:\n",
        "                A_temp = torch.clamp(A_temp, max=1.)\n",
        "            self.A_list.append(A_temp)\n",
        "\n",
        "        self.gc_list = nn.ModuleList([DynamicFilterGNN(feature_size, feature_size, self.A_list[i], bias=False) for i in range(K)])\n",
        "        hidden_size = self.feature_size\n",
        "        gc_input_size = self.feature_size * K\n",
        "\n",
        "        self.fl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "        self.Neighbor_weight = Parameter(torch.FloatTensor(feature_size))\n",
        "        stdv = 1. / math.sqrt(feature_size)\n",
        "        self.Neighbor_weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "        input_size = self.feature_size\n",
        "\n",
        "        self.rfl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ril = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rol = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rCl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # addtional vars\n",
        "        self.c = torch.nn.Parameter(torch.Tensor([1]))\n",
        "\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, 64)\n",
        "        self.fc5 = nn.Linear(64, hidden_size)\n",
        "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc8 = nn.Linear(hidden_size, 64)\n",
        "\n",
        "    def forward(self, input, Hidden_State=None, Cell_State=None, rHidden_State=None, rCell_State=None):\n",
        "        batch_size, time_steps, _ = input.shape\n",
        "        if Hidden_State is None:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if Cell_State is None:\n",
        "            Cell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rHidden_State is None:\n",
        "            rHidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rCell_State is None:\n",
        "            rCell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "\n",
        "        Hidden_State = Hidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        Cell_State = Cell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rHidden_State = rHidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rCell_State = rCell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "\n",
        "        x = input\n",
        "        gc = self.gc_list[0](x)\n",
        "        for i in range(1, self.K):\n",
        "            gc = torch.cat((gc, self.gc_list[i](x)), 1)\n",
        "\n",
        "        combined = torch.cat((gc, Hidden_State), 1)\n",
        "        dim1=combined.shape[0]\n",
        "        dim2=combined.shape[1]\n",
        "        dim3=combined.shape[2]\n",
        "        combined=combined.view(dim1,dim2//4,dim3*4)\n",
        "\n",
        "        f = torch.sigmoid(self.fl(combined))\n",
        "        i = torch.sigmoid(self.il(combined))\n",
        "        o = torch.sigmoid(self.ol(combined))\n",
        "        C = torch.tanh(self.Cl(combined))\n",
        "\n",
        "        NC = torch.mul(Cell_State,\n",
        "                       torch.mv(Variable(self.A_list[-1], requires_grad=False).cuda(), self.Neighbor_weight))\n",
        "        Cell_State = f * NC + i * C\n",
        "        Hidden_State = o * torch.tanh(Cell_State)\n",
        "\n",
        "        # LSTM\n",
        "        rcombined = torch.cat((input, rHidden_State), 1)\n",
        "        d1=rcombined.shape[0]\n",
        "        d2=rcombined.shape[1]\n",
        "        d3=rcombined.shape[2]\n",
        "        rcombined=rcombined.view(d1,d2//2,d3*2)\n",
        "        rf = torch.sigmoid(self.rfl(rcombined))\n",
        "        ri = torch.sigmoid(self.ril(rcombined))\n",
        "        ro = torch.sigmoid(self.rol(rcombined))\n",
        "        rC = torch.tanh(self.rCl(rcombined))\n",
        "        rCell_State = rf * rCell_State + ri * rC\n",
        "        rHidden_State = ro * torch.tanh(rCell_State)\n",
        "\n",
        "        # Kalman Filtering\n",
        "        var1, var2 = torch.var(input), torch.var(gc)\n",
        "\n",
        "        pred = (Hidden_State * var1 * self.c + rHidden_State * var2) / (var1 + var2 * self.c)\n",
        "\n",
        "        return pred\n",
        "        #return Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred\n",
        "\n",
        "    def Bi_torch(self, a):\n",
        "        a[a < 0] = 0\n",
        "        a[a > 0] = 1\n",
        "        return a\n",
        "\n",
        "    def loop(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        time_step = inputs.size(1)\n",
        "        Hidden_State, Cell_State, rHidden_State, rCell_State = self.initHidden(batch_size)\n",
        "        for i in range(time_step):\n",
        "            Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred = self.forward(\n",
        "                torch.squeeze(inputs[:, i:i + 1, :]), Hidden_State, Cell_State, rHidden_State, rCell_State)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "    def reinitHidden(self, batch_size, Hidden_State_data, Cell_State_data):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(Hidden_State_data, requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data, requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "# Mamba Network\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    features: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_conv: int = 4\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "    K: int = 3\n",
        "    A: torch.Tensor = None\n",
        "    feature_size: int = None\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "class KFGN_Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.encode = nn.Linear(args.features, args.d_model)\n",
        "        self.encoder_layers = nn.ModuleList([ResidualBlock(args,self.kfgn) for _ in range(args.n_layer)])\n",
        "        self.encoder_norm = RMSNorm(args.d_model)\n",
        "        # Decoder (identical to Encoder)\n",
        "        ##self.decoder_layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)]) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##self.decoder_norm = RMSNorm(args.d_model) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        self.decode = nn.Linear(args.d_model, args.features)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.encode(input_ids)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.encoder_norm(x)\n",
        "        # Decoder\n",
        "        ##for layer in self.decoder_layers:#You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##    x = layer(x) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "        ##x = self.decoder_norm(x) #You can optionally uncommand these lines to use the identical Decoder.\n",
        "\n",
        "        # Output\n",
        "        x = self.decode(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Residual Block in Mamba Model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.mixer = MambaBlock(args,kfgn)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x1 = self.norm(x)\n",
        "        x2 = self.kfgn(x1)\n",
        "        x3 = self.mixer(x2)\n",
        "        output = x3 + x1\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = kfgn\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
        "\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)\n",
        "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "\n",
        "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x):\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)\n",
        "\n",
        "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
        "\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "        # This is the new version of Selective Scan Algorithm named as \"Graph Selective Scan\"\n",
        "        #In Graph Selective Scan, we use the Feed-Forward graph information from KFGN, and incorporate the Feed-Forward information with \"delta\"\n",
        "        temp_adj = self.kfgn.gc_list[-1].get_transformed_adjacency()\n",
        "        temp_adj_padded = torch.ones(d_in, d_in, device=temp_adj.device)\n",
        "        temp_adj_padded[:temp_adj.size(0), :temp_adj.size(1)] = temp_adj\n",
        "\n",
        "        delta_p = torch.matmul(delta,temp_adj_padded)\n",
        "\n",
        "        # The fused param delta_p will participate in the following upgrading of deltaA and deltaB_u\n",
        "        deltaA = torch.exp(einsum(delta_p, A, 'b l d_in, d_in n -> b l d_in n'))\n",
        "        deltaB_u = einsum(delta_p, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "\n",
        "        y = y + u * D\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "2LGB4WYWnek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### train_STGmamba.py\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.autograd import Variable\n",
        "\n",
        "y = []  # by me\n",
        "\n",
        "def TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=1, mamba_features=307):\n",
        "    # 'mamba_features=184' if we use Knowair dataset;\n",
        "    # 'mamba_features=307' if we use PEMS04 datastet;\n",
        "    # 'mamba_features=80' if we use HZ_Metro dataset;\n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = fea_size\n",
        "\n",
        "    kfgn_mamba_args = ModelArgs(\n",
        "        K=K,\n",
        "        A=torch.Tensor(A),\n",
        "        feature_size=A.shape[0],\n",
        "        d_model=fea_size,  # hidden_dim is fea_size\n",
        "        n_layer=4,\n",
        "        features=mamba_features\n",
        "    )\n",
        "\n",
        "    kfgn_mamba = KFGN_Mamba(kfgn_mamba_args)\n",
        "    kfgn_mamba.cuda()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = optim.AdamW(kfgn_mamba.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_interval_train = []\n",
        "    losses_valid = []\n",
        "    losses_interval_valid = []\n",
        "    losses_epoch = []  # Initialize the list for epoch losses\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        trained_number = 0\n",
        "\n",
        "        valid_dataloader_iter = iter(valid_dataloader)\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if inputs.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            kfgn_mamba.zero_grad()\n",
        "\n",
        "            labels = torch.squeeze(labels)\n",
        "            pred = kfgn_mamba(inputs)  # Updated to use new model directly\n",
        "\n",
        "            loss_train = loss_MSE(pred, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            # Update learning rate by CosineAnnealingLR\n",
        "            scheduler.step()\n",
        "\n",
        "            losses_train.append(loss_train.data)\n",
        "\n",
        "            # validation\n",
        "            try:\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "            except StopIteration:\n",
        "                valid_dataloader_iter = iter(valid_dataloader)\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "            else:\n",
        "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "            labels_val = torch.squeeze(labels_val)\n",
        "\n",
        "            pred = kfgn_mamba(inputs_val)\n",
        "            loss_valid = loss_MSE(pred, labels_val)\n",
        "            losses_valid.append(loss_valid.data)\n",
        "\n",
        "            trained_number += 1\n",
        "\n",
        "            if trained_number % interval == 0:\n",
        "                cur_time = time.time()\n",
        "                loss_interval_train = np.around(sum(losses_train[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_train.append(loss_interval_train)\n",
        "                loss_interval_valid = np.around(sum(losses_valid[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_valid.append(loss_interval_valid)\n",
        "                print('Iteration #: {}, train_loss: {}, valid_loss: {}, time: {}'.format(\n",
        "                    trained_number * batch_size,\n",
        "                    loss_interval_train,\n",
        "                    loss_interval_valid,\n",
        "                    np.around([cur_time - pre_time], decimals=8)))\n",
        "                pre_time = cur_time\n",
        "\n",
        "        loss_epoch = loss_valid.cpu().data.numpy()\n",
        "        losses_epoch.append(loss_epoch)\n",
        "\n",
        "    return kfgn_mamba, [losses_train, losses_interval_train, losses_valid, losses_interval_valid]\n",
        "\n",
        "\n",
        "\n",
        "def TestSTG_Mamba(kfgn_mamba, test_dataloader, max_speed):\n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    tested_batch = 0\n",
        "\n",
        "    losses_mse = []\n",
        "    losses_l1 = []\n",
        "    MAEs = []\n",
        "    MAPEs = []\n",
        "    MSEs = []\n",
        "    RMSEs = []\n",
        "    VARs = []\n",
        "\n",
        "    #predictions = []\n",
        "    #ground_truths = []\n",
        "\n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        pred = kfgn_mamba(inputs)\n",
        "        labels = torch.squeeze(labels)\n",
        "\n",
        "        loss_mse = F.mse_loss(pred, labels)\n",
        "        loss_l1 = F.l1_loss(pred, labels)\n",
        "        MAE = torch.mean(torch.abs(pred - torch.squeeze(labels)))\n",
        "        MAPE = torch.mean(torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels)))\n",
        "        # Calculate MAPE only for non-zero labels\n",
        "        non_zero_labels = torch.abs(labels) > 0\n",
        "        if torch.any(non_zero_labels):\n",
        "            MAPE_values = torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels))\n",
        "            MAPE = torch.mean(MAPE_values[non_zero_labels])\n",
        "            MAPEs.append(MAPE.item())\n",
        "\n",
        "        MSE = torch.mean((torch.squeeze(labels) - pred)**2)\n",
        "        RMSE = math.sqrt(torch.mean((torch.squeeze(labels) - pred)**2))\n",
        "        VAR = 1-(torch.var(torch.squeeze(labels)-pred))/torch.var(torch.squeeze(labels))\n",
        "\n",
        "        losses_mse.append(loss_mse.item())\n",
        "        losses_l1.append(loss_l1.item())\n",
        "        MAEs.append(MAE.item())\n",
        "        MAPEs.append(MAPE.item())\n",
        "        MSEs.append(MSE.item())\n",
        "        RMSEs.append(RMSE)\n",
        "        VARs.append(VAR.item())\n",
        "\n",
        "        # Reshape pred to 2D before creating DataFrame\n",
        "        #predictions.append(pd.DataFrame(pred.cpu().data.numpy().reshape(-1, fea_size)))\n",
        "        #ground_truths.append(pd.DataFrame(labels.cpu().data.numpy()))\n",
        "        y.append(pred.cpu().data.numpy())  # by me\n",
        "\n",
        "        tested_batch += 1\n",
        "\n",
        "        if tested_batch % 100 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format(\n",
        "                tested_batch * batch_size,\n",
        "                np.around([loss_l1.data[0]], decimals=8),\n",
        "                np.around([loss_mse.data[0]], decimals=8),\n",
        "                np.around([cur_time - pre_time], decimals=8)))\n",
        "            pre_time = cur_time\n",
        "\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    MAEs = np.array(MAEs)\n",
        "    MAPEs = np.array(MAPEs)\n",
        "    MSEs = np.array(MSEs)\n",
        "    RMSEs = np.array(RMSEs)\n",
        "    VARs = np.array(VARs)\n",
        "\n",
        "    mean_l1 = np.mean(losses_l1) * max_speed\n",
        "    std_l1 = np.std(losses_l1) * max_speed\n",
        "    mean_mse = np.mean(losses_mse) * max_speed\n",
        "    MAE_ = np.mean(MAEs) * max_speed\n",
        "    std_MAE_ = np.std(MAEs) * max_speed #std_MAE measures the consistency & stability of the model's performance across different test sets or iterations. Usually if (std_MAE/MSE)<=10%., means the trained model is good.\n",
        "    MAPE_ = np.mean(MAPEs) * 100\n",
        "    MSE_ = np.mean(MSEs) * (max_speed ** 2)\n",
        "    RMSE_ = np.mean(RMSEs) * max_speed\n",
        "    VAR_ = np.mean(VARs)\n",
        "    results = [MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_]\n",
        "\n",
        "    print('Tested: MAE: {}, std_MAE: {}, MAPE: {}, MSE: {}, RMSE: {}, VAR: {}'.format(MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_))\n",
        "    return results"
      ],
      "metadata": {
        "id": "AB_S2LDJnVWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\nLoading PEMS04 data...\")\n",
        "speed_matrix = pd.read_csv('pems04_flow.csv',sep=',')\n",
        "A = np.load('pems04_adj.npy')\n",
        "\n",
        "\n",
        "print(\"\\nPreparing train/test data...\")\n",
        "train_dataloader, valid_dataloader, test_dataloader, max_value = PrepareDataset(speed_matrix, BATCH_SIZE=48)\n",
        "\n",
        "print(\"\\nTraining STGmamba model...\")\n",
        "STGmamba, STGmamba_loss = TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=50, mamba_features=307)\n",
        "print(\"\\nTesting STGmamba model...\")\n",
        "results = TestSTG_Mamba(STGmamba, test_dataloader, max_value)"
      ],
      "metadata": {
        "id": "z4YRyEW8e_-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ad11c7-0542-49b5-bce2-79eacadce1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading PEMS04 data...\n",
            "\n",
            "Preparing train/test data...\n",
            "\n",
            "Training STGmamba model...\n",
            "Epoch 0/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.05330336093902588, valid_loss: 0.05025235936045647, time: [15.58357]\n",
            "Epoch 1/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.008394859731197357, valid_loss: 0.008907579816877842, time: [15.07901859]\n",
            "Epoch 2/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.005456509999930859, valid_loss: 0.005720220040529966, time: [15.13301563]\n",
            "Epoch 3/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00375872990116477, valid_loss: 0.00395190017297864, time: [15.33652544]\n",
            "Epoch 4/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.003300759941339493, valid_loss: 0.003467259928584099, time: [15.05442047]\n",
            "Epoch 5/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.003022179938852787, valid_loss: 0.0031965100206434727, time: [15.12102938]\n",
            "Epoch 6/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0028603700920939445, valid_loss: 0.003016510047018528, time: [15.28520346]\n",
            "Epoch 7/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002763319993391633, valid_loss: 0.00293691991828382, time: [15.03752208]\n",
            "Epoch 8/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002706069964915514, valid_loss: 0.0028727399185299873, time: [15.08345461]\n",
            "Epoch 9/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0025907899253070354, valid_loss: 0.002776829991489649, time: [18.05223894]\n",
            "Epoch 10/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002513420069590211, valid_loss: 0.002703239908441901, time: [15.08908987]\n",
            "Epoch 11/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0024477799888700247, valid_loss: 0.002651900053024292, time: [15.15754819]\n",
            "Epoch 12/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0024089699145406485, valid_loss: 0.0026284900959581137, time: [15.08562779]\n",
            "Epoch 13/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0023725400678813457, valid_loss: 0.0026204900350421667, time: [15.73287845]\n",
            "Epoch 14/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002328980015590787, valid_loss: 0.0025740000419318676, time: [15.16600537]\n",
            "Epoch 15/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002302400069311261, valid_loss: 0.0025615498889237642, time: [15.32899332]\n",
            "Epoch 16/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002242960035800934, valid_loss: 0.002500629983842373, time: [15.28388166]\n",
            "Epoch 17/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0022099700290709734, valid_loss: 0.002480460098013282, time: [15.26427007]\n",
            "Epoch 18/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021717699710279703, valid_loss: 0.0024504000321030617, time: [15.18934774]\n",
            "Epoch 19/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021504799369722605, valid_loss: 0.0024383000563830137, time: [15.13934445]\n",
            "Epoch 20/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002112919930368662, valid_loss: 0.0024226598907262087, time: [15.31774139]\n",
            "Epoch 21/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021085499320179224, valid_loss: 0.0024317800998687744, time: [15.33385992]\n",
            "Epoch 22/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020949500612914562, valid_loss: 0.002404090017080307, time: [15.48073363]\n",
            "Epoch 23/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020524701103568077, valid_loss: 0.0023728099185973406, time: [15.3301661]\n",
            "Epoch 24/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002011609962210059, valid_loss: 0.0023588899057358503, time: [15.69236803]\n",
            "Epoch 25/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002001279965043068, valid_loss: 0.0023423898965120316, time: [15.33015919]\n",
            "Epoch 26/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020206000190228224, valid_loss: 0.0023741798941046, time: [15.24994373]\n",
            "Epoch 27/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001975039951503277, valid_loss: 0.0023428599815815687, time: [15.46570182]\n",
            "Epoch 28/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019512299913913012, valid_loss: 0.0023296100553125143, time: [15.58076167]\n",
            "Epoch 29/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019166399724781513, valid_loss: 0.0022952600847929716, time: [15.39677525]\n",
            "Epoch 30/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018953799735754728, valid_loss: 0.0022805400658398867, time: [15.48026633]\n",
            "Epoch 31/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001894650049507618, valid_loss: 0.0023017399944365025, time: [15.64206052]\n",
            "Epoch 32/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018830300541594625, valid_loss: 0.0022849899251013994, time: [15.36098814]\n",
            "Epoch 33/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018538200529292226, valid_loss: 0.0022666500881314278, time: [15.26320601]\n",
            "Epoch 34/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018320600502192974, valid_loss: 0.0022527100518345833, time: [15.39794612]\n",
            "Epoch 35/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001807959983125329, valid_loss: 0.0022373099345713854, time: [15.34810567]\n",
            "Epoch 36/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018059699796140194, valid_loss: 0.002248130040243268, time: [15.34171295]\n",
            "Epoch 37/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017973700305446982, valid_loss: 0.002237220061942935, time: [15.29625249]\n",
            "Epoch 38/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017733200220391154, valid_loss: 0.0022154198959469795, time: [15.57694221]\n",
            "Epoch 39/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017648700159043074, valid_loss: 0.0022161200176924467, time: [15.30575109]\n",
            "Epoch 40/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00173590995837003, valid_loss: 0.002201500115916133, time: [15.49678016]\n",
            "Epoch 41/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017173599917441607, valid_loss: 0.0021864199079573154, time: [15.46424651]\n",
            "Epoch 42/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016936600441113114, valid_loss: 0.0021753599867224693, time: [15.39504242]\n",
            "Epoch 43/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017633100505918264, valid_loss: 0.00224695005454123, time: [15.26116061]\n",
            "Epoch 44/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016800799639895558, valid_loss: 0.002160989912226796, time: [15.34314084]\n",
            "Epoch 45/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016533300513401628, valid_loss: 0.002147410064935684, time: [15.50618577]\n",
            "Epoch 46/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016428299713879824, valid_loss: 0.0021422700956463814, time: [15.32163596]\n",
            "Epoch 47/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016378599684685469, valid_loss: 0.0021359799429774284, time: [15.35201931]\n",
            "Epoch 48/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016448399983346462, valid_loss: 0.0021460799034684896, time: [15.35743761]\n",
            "Epoch 49/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016267499886453152, valid_loss: 0.0021365899592638016, time: [15.53690219]\n",
            "\n",
            "Testing STGmamba model...\n",
            "Tested: MAE: 22.31216552057143, std_MAE: 0.8865823040592054, MAPE: 18.73235445598076, MSE: 1169.0933826095463, RMSE: 34.17178833467672, VAR: 0.9441388352163906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi_AYn8nlDiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}